{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7360,"status":"ok","timestamp":1732602930576,"user":{"displayName":"Akshat Gurbuxani","userId":"17483033598681941504"},"user_tz":300},"id":"3k3HV15We-jE"},"outputs":[],"source":["import os\n","import json\n","import re\n","from langchain_community.document_loaders import PyPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","import spacy\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1244,"status":"ok","timestamp":1732602931818,"user":{"displayName":"Akshat Gurbuxani","userId":"17483033598681941504"},"user_tz":300},"id":"HArI7OzVgU6M"},"outputs":[],"source":["# Load spaCy model\n","nlp = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":563,"status":"ok","timestamp":1732604057887,"user":{"displayName":"Akshat Gurbuxani","userId":"17483033598681941504"},"user_tz":300},"id":"RIuO3bM3gHd_","outputId":"4fd04bd8-c01b-4172-9bc0-9eb3eb581707"},"outputs":[{"name":"stdout","output_type":"stream","text":["All text file chunks have been saved to chunked_data_all_text_files_with_links.json\n","Number of text files processed: 189\n"]}],"source":["\n","\n","# Path to the main dataset directory containing text files\n","dataset_path = '../data/data_txt1'\n","dataset_links_path = '../data/data_txt/_dataset_links/dataset_links.txt'\n","\n","# Initialize list to store all chunks\n","all_chunks = []\n","text_file_count = 0\n","\n","# Function to clean file names\n","def clean_name(name):\n","    # Remove parentheses and their content\n","    name = re.sub(r'\\s*\\([^)]*\\)', '', name)\n","    # Remove .txt extension\n","    name = re.sub(r'\\.txt$', '', name, flags=re.IGNORECASE)\n","    return name.strip()\n","\n","# Load dataset links into a dictionary\n","def load_links(file_path):\n","    links_dict = {}\n","    with open(file_path, 'r') as f:\n","        for line in f:\n","            if '.pdf:' in line:\n","                file_name, link = line.split('.pdf:')\n","                links_dict[clean_name(file_name.strip())] = link.strip()\n","    return links_dict\n","\n","# Load links\n","file_links = load_links(dataset_links_path)\n","\n","# print(file_links)\n","\n","# Function to process text files\n","def process_text_file(file_path, folder_name):\n","    global text_file_count\n","    try:\n","        # Read the content of the text file\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            text_content = f.read()\n","\n","        # Check if the text file has content\n","        if not text_content.strip():\n","            print(f\"Warning: No content found in {file_path}\")\n","            return\n","\n","        # Initialize the text splitter\n","        text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=700,\n","            chunk_overlap=50,\n","        )\n","\n","        # Split the text content into chunks\n","        chunks = text_splitter.split_text(text_content)\n","\n","        # Check if the text was split correctly\n","        if not chunks:\n","            print(f\"Warning: No chunks created from {file_path}\")\n","            return\n","\n","        # Clean the file name and folder name\n","        clean_folder_name = clean_name(folder_name)\n","        clean_file_name = clean_name(os.path.basename(file_path))\n","\n","        # path = os.path.basename(file_path).strip().replace('.txt', '')\n","\n","        # print(path)\n","\n","        # Retrieve the link for the file\n","        file_uri = file_links.get(clean_file_name, None)\n","        # print(file_links)\n","        # print(file_uri)\n","        # print(path)\n","        #print(file_links.keys())\n","        if file_uri is None:\n","            print(f\"Warning: No link found for {clean_file_name}\")\n","            return\n","\n","        # Prepare the chunks in a dictionary format\n","        for i, chunk in enumerate(chunks):\n","            chunk_data = {\n","                'folder_name': clean_folder_name,\n","                'file_name': clean_file_name,\n","                'chunk_id': i + 1,\n","                'uri': file_uri,  # Add the link as metadata\n","                'content': chunk\n","            }\n","            all_chunks.append(chunk_data)\n","        text_file_count += 1\n","        # print(f\"Processed {file_path}, created {len(chunks)} chunks.\")\n","    except Exception as e:\n","        print(f\"Error processing {file_path}: {str(e)}\")\n","\n","# # Process all text files in the dataset path\n","# for file_name in os.listdir(dataset_path):\n","#     if file_name.endswith('.txt'):\n","#         file_path = os.path.join(dataset_path, file_name)\n","#         folder_name = os.path.basename(root)\n","#         process_text_file(file_path, folder_name)\n","\n","# Walk through the main dataset directory and process all PDF files\n","for root, dirs, files in os.walk(dataset_path):\n","    for file_name in files:\n","        if file_name.endswith('.txt'):\n","            file_path = os.path.join(root, file_name)\n","            folder_name = os.path.basename(root)  # Get the folder name for indexing\n","            process_text_file(file_path, folder_name)\n","\n","# Check if there are any chunks to save\n","if all_chunks:\n","    # Save all chunks to a JSON file\n","    output_path = '../data/data_json/chunked_data_all_text_files_with_links.json'\n","    with open(output_path, 'w', encoding='utf-8') as json_file:\n","        json.dump(all_chunks, json_file, indent=4, ensure_ascii=False)\n","    print(f\"All text file chunks have been saved to {output_path}\")\n","else:\n","    print(\"No chunks were created. Please check the input files.\")\n","\n","# Print the number of text files processed\n","print(f\"Number of text files processed: {text_file_count}\")\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
